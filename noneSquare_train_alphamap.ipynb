{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from modelAlpha import _netlocalWD,_netWG\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt():\n",
    "    def __init__(self):\n",
    "        self.pngdataroot = 'dataset/pngset'\n",
    "        self.dataroot='dataset/animation'\n",
    "        self.workers=2\n",
    "        self.batchSize=128 #'input batch size')\n",
    "        self.imageSize=256 #the height / width of the input image to network')\n",
    "        self.nz=100\n",
    "        self.ngf=128\n",
    "        self.ndf=128 # center image size\n",
    "        self.nc=4\n",
    "        self.niter=4000\n",
    "        self.lr=0.0002\n",
    "        self.beta1=0.5\n",
    "        self.cuda=True\n",
    "        self.ngpu=1\n",
    "        #self.netG=''\n",
    "        #self.netD=''\n",
    "        self.netG='model/netG_streetview_1.pth'\n",
    "        self.netD='model/netlocalD_1.pth'\n",
    "        self.outf='.'\n",
    "        self.manualSeed = 0\n",
    "        self.nBottleneck=4000  # 'of dim for bottleneck of encoder')\n",
    "        self.overlapPred = 0 # 'overlapping edges')\n",
    "        self.nef = 64#'of encoder filters in first conv layer')\n",
    "        self.wtl2 = 0.998 #'0 means do not use else use with this weight')\n",
    "        self.wtlD =0.001# means do not use else use with this weight')\n",
    "        self.jittering=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  0\n"
     ]
    }
   ],
   "source": [
    "if opt.manualSeed is None:\n",
    "        opt.manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", opt.manualSeed)\n",
    "random.seed(opt.manualSeed)\n",
    "torch.manual_seed(opt.manualSeed)\n",
    "if opt.cuda:\n",
    "    torch.cuda.manual_seed_all(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading has been done\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() and not opt.cuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(opt.imageSize+64),\n",
    "                                transforms.RandomCrop(opt.imageSize, padding=0),\n",
    "                                transforms.ToTensor()])\n",
    "transform_png = transforms.Compose([transforms.Resize(opt.ndf),\n",
    "                                    transforms.RandomRotation((-20,20)),\n",
    "                                    transforms.RandomVerticalFlip(),\n",
    "                                    transforms.RandomCrop(opt.ndf, padding=0),\n",
    "                                    transforms.ToTensor()])\n",
    "dataset = dset.ImageFolder(root=opt.dataroot, transform=transform)\n",
    "pngdataset = dset.ImageFolder(root=opt.pngdataroot, transform=transform_png)\n",
    "assert dataset\n",
    "assert pngdataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n",
    "                                         shuffle=True, num_workers=int(opt.workers))\n",
    "pngdataloader = torch.utils.data.DataLoader(pngdataset, batch_size=opt.batchSize,\n",
    "                                          shuffle=True, num_workers=int(opt.workers))\n",
    "\n",
    "print(\"data loading has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting options has been done\n"
     ]
    }
   ],
   "source": [
    "ngpu = int(opt.ngpu)\n",
    "ngf = int(opt.ngf)\n",
    "ndf = int(opt.ndf)\n",
    "nc = 3\n",
    "nef = int(opt.nef)\n",
    "nBottleneck = int(opt.nBottleneck)\n",
    "wtl2 = float(opt.wtl2)\n",
    "overlapL2Weight = 10\n",
    "\n",
    "print(\"Setting options has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netWG(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU(0.2, inplace)\n",
      "    (11): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (13): LeakyReLU(0.2, inplace)\n",
      "    (14): Conv2d(512, 4000, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(4000, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (16): LeakyReLU(0.2, inplace)\n",
      "    (17): ConvTranspose2d(4000, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (18): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (19): ReLU(inplace)\n",
      "    (20): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (22): ReLU(inplace)\n",
      "    (23): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (25): ReLU(inplace)\n",
      "    (26): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (27): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (28): ReLU(inplace)\n",
      "    (29): ConvTranspose2d(128, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (30): Tanh()\n",
      "  )\n",
      ")\n",
      "128\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "resume_epoch=0\n",
    "\n",
    "netG = _netWG(opt)\n",
    "netG.apply(weights_init)\n",
    "if opt.netG != '':\n",
    "    netG.load_state_dict(torch.load(opt.netG,map_location=lambda storage, location: storage)['state_dict'])\n",
    "    resume_epoch = torch.load(opt.netG)['epoch']\n",
    "print(netG)\n",
    "\n",
    "\n",
    "netD = _netlocalWD(opt)\n",
    "netD.apply(weights_init)\n",
    "if opt.netD != '':\n",
    "    netD.load_state_dict(torch.load(opt.netD,map_location=lambda storage, location: storage)['state_dict'])\n",
    "    resume_epoch = torch.load(opt.netD)['epoch']\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "criterionMSE = nn.MSELoss()\n",
    "\n",
    "input_real = torch.FloatTensor(opt.batchSize, 4, opt.imageSize, opt.imageSize)\n",
    "input_cropped = torch.FloatTensor(opt.batchSize, 4, opt.imageSize, opt.imageSize)\n",
    "input_png = torch.FloatTensor(opt.batchSize,1, opt.ndf, opt.ndf)\n",
    "input_pngReverse = torch.FloatTensor(opt.batchSize,4, opt.ndf, opt.ndf)\n",
    "label = torch.FloatTensor(opt.batchSize)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "image_margin = int((opt.imageSize - opt.ndf)/2)\n",
    "\n",
    "print(opt.batchSize)\n",
    "print(opt.imageSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Optimizer has been done\n"
     ]
    }
   ],
   "source": [
    "real_center = torch.FloatTensor(opt.batchSize, 4, opt.ndf, opt.ndf)\n",
    "#real_center = torch.FloatTensor(64, 3, 64,64)\n",
    "\n",
    "if opt.cuda:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    criterionMSE.cuda()\n",
    "    input_real, input_cropped,label = input_real.cuda(),input_cropped.cuda(), label.cuda()\n",
    "    real_center = real_center.cuda()\n",
    "    input_png = input_png.cuda()\n",
    "\n",
    "\n",
    "input_real = Variable(input_real)\n",
    "input_cropped = Variable(input_cropped)\n",
    "label = Variable(label)\n",
    "input_png = Variable(input_png)\n",
    "input_pngReverse = Variable(input_pngReverse)\n",
    "\n",
    "\n",
    "real_center = Variable(real_center)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "\n",
    "print(\"Setup Optimizer has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideaconcert_ai\\Anaconda3\\envs\\py\\lib\\site-packages\\torch\\nn\\functional.py:1189: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "C:\\Users\\ideaconcert_ai\\Anaconda3\\envs\\py\\lib\\site-packages\\torch\\nn\\functional.py:1189: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3002/4000\n",
      "3003/4000\n",
      "3004/4000\n",
      "3005/4000\n",
      "3006/4000\n",
      "3007/4000\n",
      "3008/4000\n",
      "3009/4000\n",
      "3010/4000\n",
      "3011/4000\n",
      "3012/4000\n",
      "3013/4000\n",
      "3014/4000\n",
      "3015/4000\n",
      "3016/4000\n",
      "3017/4000\n",
      "3018/4000\n",
      "3019/4000\n",
      "3020/4000\n",
      "3021/4000\n",
      "3022/4000\n",
      "3023/4000\n",
      "3024/4000\n",
      "3025/4000\n",
      "3026/4000\n",
      "3027/4000\n",
      "3028/4000\n",
      "3029/4000\n",
      "3030/4000\n",
      "3031/4000\n",
      "3032/4000\n",
      "3033/4000\n",
      "3034/4000\n",
      "3035/4000\n",
      "3036/4000\n",
      "3037/4000\n",
      "3038/4000\n",
      "3039/4000\n",
      "3040/4000\n",
      "3041/4000\n",
      "3042/4000\n",
      "3043/4000\n",
      "3044/4000\n",
      "3045/4000\n",
      "3046/4000\n",
      "3047/4000\n",
      "3048/4000\n",
      "3049/4000\n",
      "3050/4000\n",
      "3051/4000\n"
     ]
    }
   ],
   "source": [
    "pngmx = 93\n",
    "for epoch in range(resume_epoch,opt.niter):      \n",
    "    # jittering add\n",
    "    randwf = random.uniform(-1.0,1.0)\n",
    "    randhf = random.uniform(-1.0,1.0)\n",
    "    if opt.jittering:\n",
    "        jitterSizeW = int(opt.imageSize/5*randwf)\n",
    "        jitterSizeH = int(opt.imageSize/5*randhf)\n",
    "        print(\"jittering : W > \",jitterSizeW,\" H >\",jitterSizeH)\n",
    "    else :\n",
    "        jitterSizeW = 0\n",
    "        jitterSizeH = 0\n",
    "\n",
    "    pngdata = list(enumerate(pngdataloader, 0))[0][1][0]\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        png_image = pngdata\n",
    "        #print(png_image.size())\n",
    "        real_cpu, _ = data\n",
    "        real_cpuV =  Variable(real_cpu)\n",
    "        real_center_cpu = real_cpu[:,:,\n",
    "                                   int(image_margin+jitterSizeW):int(image_margin+opt.ndf+jitterSizeW),\n",
    "                                   int(image_margin+jitterSizeH):int(image_margin+opt.ndf+jitterSizeH)]\n",
    "        batch_size = real_cpu.size(0)\n",
    "        input_real.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "        #input_cropped.data.resize_(real_cpu.size()).copy_(real_cpu)\n",
    "        #real_center.data.resize_(real_center_cpu.size()).copy_(real_center_cpu)\n",
    "        \n",
    "        input_cropped.data.resize_(torch.Size([batch_size, opt.nc, opt.imageSize, opt.imageSize]))\n",
    "        real_center.data.resize_(torch.Size([batch_size, opt.nc, opt.ndf, opt.ndf]))\n",
    "        #print(\"batch_size : \",batch_size , \" input_cropped : \",input_cropped.data.size())\n",
    "        #print(\"real_cetner\", real_center.data[:,0].size(), \" real_cetner_cpu\", real_center_cpu[:,0].size())\n",
    "        for j in range(0,batch_size):\n",
    "            real_center.data[j,0] = real_center_cpu[j,0]\n",
    "            real_center.data[j,1] = real_center_cpu[j,1]\n",
    "            real_center.data[j,2] = real_center_cpu[j,2]\n",
    "            real_center.data[j,3] = 1\n",
    "            input_cropped.data[j,0] = real_cpuV.data[j,0]\n",
    "            input_cropped.data[j,1] = real_cpuV.data[j,1]\n",
    "            input_cropped.data[j,2] = real_cpuV.data[j,2]\n",
    "            input_cropped.data[j,3] = 1\n",
    "            input_png.data[j%pngmx,0] = torch.abs(png_image[j%pngmx,0] - 1)\n",
    "            input_pngReverse.data[j%pngmx,0] = png_image[j%pngmx,0]\n",
    "            input_pngReverse.data[j%pngmx,1] = png_image[j%pngmx,0]\n",
    "            input_pngReverse.data[j%pngmx,2] = png_image[j%pngmx,0]\n",
    "            input_pngReverse.data[j%pngmx,3] = png_image[j%pngmx,0]\n",
    "            \n",
    "            input_cropped.data[j,0,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)] = input_png.data[j%pngmx,0] * input_cropped.data[j,0,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)]\n",
    "            input_cropped.data[j,1,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)] = input_png.data[j%pngmx,0] * input_cropped.data[j,1,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)]\n",
    "            input_cropped.data[j,2,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)] = input_png.data[j%pngmx,0] * input_cropped.data[j,2,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)]\n",
    "            input_cropped.data[j,3,\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeW):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeW),\n",
    "                   int(image_margin+opt.overlapPred+jitterSizeH):int(image_margin+opt.ndf-opt.overlapPred+jitterSizeH)] = input_png.data[j%pngmx,0]\n",
    "        \n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        label.data.resize_(batch_size).fill_(real_label)\n",
    "        \n",
    "        #print(\"real_center size :\",real_center.size(),\", label size:\",label.data.size())\n",
    "        output = netD(real_center)\n",
    "        #print(\"output size:\",output.size())\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "        fake = netG(input_cropped)\n",
    "        label.data.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG_D = criterion(output, label)\n",
    "        # errG_D.backward(retain_variables=True)\n",
    "\n",
    "        # errG_l2 = criterionMSE(fake,real_center)\n",
    "        wtl2Matrix = real_center.clone()\n",
    "        wtl2Matrix.data.fill_(wtl2*overlapL2Weight)\n",
    "        \n",
    "        '''\n",
    "        for j in range(0,batch_size):\n",
    "            wtl2Matrix.data[j,:,\n",
    "                            int(opt.overlapPred):int(opt.ndf - opt.overlapPred),\n",
    "                            int(opt.overlapPred):int(opt.ndf - opt.overlapPred)] = wtl2 * input_pngReverse.data\n",
    "        '''\n",
    "\n",
    "        errG_l2 = (fake-real_center).pow(2)\n",
    "        errG_l2 = errG_l2 * wtl2Matrix\n",
    "        errG_l2 = errG_l2.mean()\n",
    "\n",
    "        errG = (1-wtl2) * errG_D + wtl2 * errG_l2\n",
    "\n",
    "        errG.backward()\n",
    "\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "        '''\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f / %.4f l_D(x): %.4f l_D(G(z)): %.4f'\n",
    "              % (epoch, opt.niter, i, len(dataloader),\n",
    "                 errD.data[0], errG_D.data[0],errG_l2.data[0], D_x,D_G_z1, ))\n",
    "        '''\n",
    "        if i % 50 == 0:\n",
    "            vutils.save_image(real_cpu,'result/train/real/real_samples_epoch_%03d.png' % (epoch))            \n",
    "            tmpsave = torch.FloatTensor(batch_size, 3, opt.imageSize, opt.imageSize)\n",
    "            tmpsave[:,0] = input_cropped.data[:,0]\n",
    "            tmpsave[:,1] = input_cropped.data[:,1]\n",
    "            tmpsave[:,2] = input_cropped.data[:,2]\n",
    "            vutils.save_image(tmpsave,'result/train/cropped/cropped_samples_epoch_%03d.png' % (epoch))\n",
    "            '''\n",
    "            tmpsave[:,0] = input_cropped.data[:,0]\n",
    "            tmpsave[:,1] = input_cropped.data[:,1]\n",
    "            tmpsave[:,2] = input_cropped.data[:,2]\n",
    "            tmpfake = torch.FloatTensor(batch_size, 3, opt.ndf, opt.ndf)\n",
    "            tmpReverse = torch.FloatTensor(3, opt.ndf, opt.ndf)\n",
    "            tmpfake[:,0]= fake.data[:,0]\n",
    "            tmpfake[:,1]= fake.data[:,1]\n",
    "            tmpfake[:,2]= fake.data[:,2]\n",
    "            tmpReverse[0] = input_pngReverse.data[0]\n",
    "            #print(tmpfake.size(), \" \",fake.size())\n",
    "            \n",
    "            #print(fake.data[0,2].size(), input_pngReverse.data[2].size())\n",
    "            #print((fake.data[0,2]).size())\n",
    "            tmpsave[:,0,\n",
    "                 int(opt.imageSize/4+jitterSizeW):int(opt.imageSize/4+opt.imageSize/2+jitterSizeW),\n",
    "                 int(opt.imageSize/4+jitterSizeH):int(opt.imageSize/4+opt.imageSize/2+jitterSizeH)] += tmpfake[:,0] * tmpReverse[0]\n",
    "            tmpsave[:,1,\n",
    "                 int(opt.imageSize/4+jitterSizeW):int(opt.imageSize/4+opt.imageSize/2+jitterSizeW),\n",
    "                 int(opt.imageSize/4+jitterSizeH):int(opt.imageSize/4+opt.imageSize/2+jitterSizeH)] += tmpfake[:,1] * tmpReverse[0]\n",
    "            tmpsave[:,2,\n",
    "                 int(opt.imageSize/4+jitterSizeW):int(opt.imageSize/4+opt.imageSize/2+jitterSizeW),\n",
    "                 int(opt.imageSize/4+jitterSizeH):int(opt.imageSize/4+opt.imageSize/2+jitterSizeH)] += tmpfake[:,2] * tmpReverse[0]  \n",
    "            #vutils.save_image(tmpsave,'result/train/masked/masked_center_samples_epoch_%03d.png' % (epoch))\n",
    "            '''\n",
    "            tmpsave[:,0,\n",
    "                 int(opt.imageSize/4+jitterSizeW):int(opt.imageSize/4+opt.imageSize/2+jitterSizeW),\n",
    "                 int(opt.imageSize/4+jitterSizeH):int(opt.imageSize/4+opt.imageSize/2+jitterSizeH)] = fake.data[:,0]\n",
    "            tmpsave[:,1,\n",
    "                 int(opt.imageSize/4+jitterSizeW):int(opt.imageSize/4+opt.imageSize/2+jitterSizeW),\n",
    "                 int(opt.imageSize/4+jitterSizeH):int(opt.imageSize/4+opt.imageSize/2+jitterSizeH)] = fake.data[:,1]\n",
    "            tmpsave[:,2,\n",
    "                 int(opt.imageSize/4+jitterSizeW):int(opt.imageSize/4+opt.imageSize/2+jitterSizeW),\n",
    "                 int(opt.imageSize/4+jitterSizeH):int(opt.imageSize/4+opt.imageSize/2+jitterSizeH)] = fake.data[:,2]\n",
    "            vutils.save_image(tmpsave,'result/train/recon/recon_center_samples_epoch_%03d.png' % (epoch))\n",
    "\n",
    "    # do checkpointing\n",
    "    print('%d/%d' % (epoch, opt.niter))\n",
    "    torch.save({'epoch':epoch+1,\n",
    "                'state_dict':netG.state_dict()},\n",
    "                'model/netG_streetview_{0}.pth'.format(epoch%2) )\n",
    "    torch.save({'epoch':epoch+1,\n",
    "                'state_dict':netD.state_dict()},\n",
    "                'model/netlocalD_{0}.pth'.format(epoch%2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
