{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "#from modelAlpha import _netlocalWD,_netWG\n",
    "from modelAlpha128 import _netlocalWD,_netWG\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt():\n",
    "    def __init__(self):\n",
    "        self.testimg = 'sao0.png' # 테스트 할 이미지\n",
    "        #self.testimg = 'oj_cropped_sample2.png'\n",
    "        self.workers=2\n",
    "        self.batchSize=64 #'input batch size') 배치사이즈\n",
    "        self.imageSize=128 #the height / width of the input image to network') # 이미지사이즈\n",
    "        self.nz=100\n",
    "        self.ngf=128\n",
    "        self.ndf=128 # center image size 중앙 천공영역 사이즈\n",
    "        self.nc=4 # 이미지 채널 갯수 (RGBA)\n",
    "        self.niter=1\n",
    "        self.lr=0.0002 #러닝레이트\n",
    "        self.beta1=0.5\n",
    "        self.cuda=True\n",
    "        self.ngpu=1\n",
    "        #self.netG='model/zibriNetG7000.pth' #테스트할 네트워크\n",
    "        self.netG='model/netG_streetview_1.pth' #테스트할 네트워크\n",
    "        #self.netG='model/tmp.pth'\n",
    "        self.outf='.'\n",
    "        self.manualSeed = 0\n",
    "        self.nBottleneck=4000  # 'of dim for bottleneck of encoder') 네트워크 바틀넥 깊이\n",
    "        self.overlapPred = 0 # 'overlapping edges') 천공영역 오버랩핑 사이즈\n",
    "        self.nef = 128#'of encoder filters in first conv layer') # 네트워크 필터의 갯수\n",
    "        self.wtl2 = 0.998 #'0 means do not use else use with this weight')\n",
    "        self.wtlD =0.001# means do not use else use with this weight')\n",
    "        self.jittering=False # 천공영역 jittering 여부\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt 클래스 오브젝트 생성\n",
    "opt = opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netWG(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(4, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU(0.2, inplace)\n",
      "    (11): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (13): LeakyReLU(0.2, inplace)\n",
      "    (14): Conv2d(1024, 4000, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(4000, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (16): LeakyReLU(0.2, inplace)\n",
      "    (17): ConvTranspose2d(4000, 2048, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (18): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (19): ReLU(inplace)\n",
      "    (20): ConvTranspose2d(2048, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (21): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (22): ReLU(inplace)\n",
      "    (23): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (24): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (25): ReLU(inplace)\n",
      "    (26): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (27): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (28): ReLU(inplace)\n",
      "    (29): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (30): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (31): ReLU(inplace)\n",
      "    (32): ConvTranspose2d(128, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (33): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "# netG와 그 파라메터 블러옴\n",
    "netG = _netWG(opt)\n",
    "netG.apply(weights_init)\n",
    "netG.load_state_dict(torch.load(opt.netG,map_location=lambda storage, location: storage)['state_dict'])\n",
    "resume_epoch = torch.load(opt.netG)['epoch']\n",
    "netG.eval()\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading has been done\n"
     ]
    }
   ],
   "source": [
    "#image_margin = int((opt.imageSize - opt.ndf)/2)\n",
    "image_margin = 0 #이미지 마진크기\n",
    "transform = transforms.Compose([transforms.ToTensor()]) #데이터셋 트랜스폼 정의\n",
    "\n",
    "png_image = utils.load_image(opt.testimg, opt.imageSize) #테스트 이미지 가져오기\n",
    "png_image = transform(png_image) # 이미지 트랜스폼\n",
    "input_cropped = torch.FloatTensor(1, 4, opt.imageSize, opt.imageSize) # 천공이미지 들어갈 텐서 정의\n",
    "input_pngReverse = torch.FloatTensor(4, opt.ndf, opt.ndf) # 천공이미지 뒤집은 이미지들어갈 텐서 정의\n",
    "result_img = torch.FloatTensor(1, 3, opt.imageSize, opt.imageSize) # 결과이미지 텐서\n",
    "input_cropped = Variable(input_cropped)\n",
    "\n",
    "\n",
    "#이미지를 변수에 담음\n",
    "#input_cropped.data.resize_(png_image.size()).copy_(png_image)\n",
    "input_pngReverse[0] = png_image[3][image_margin:image_margin+opt.ndf, image_margin:image_margin+opt.ndf]\n",
    "png_imageV = Variable(png_image)\n",
    "input_cropped.data[0,0] = png_imageV.data[0]\n",
    "input_cropped.data[0,1] = png_imageV.data[1]\n",
    "input_cropped.data[0,2] = png_imageV.data[2]\n",
    "#원본 이미지 저장\n",
    "vutils.save_image(input_pngReverse[0],'result/single/cropped.png')\n",
    "#Hadamard Product으로 천공영역텐서에 담기\n",
    "input_cropped.data[0,0,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf] = input_pngReverse[0] * input_cropped.data[0,0,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf]\n",
    "input_cropped.data[0,1,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf] = input_pngReverse[0] * input_cropped.data[0,1,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf]\n",
    "input_cropped.data[0,2,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf] = input_pngReverse[0] * input_cropped.data[0,2,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf]\n",
    "input_cropped.data[0,3] = png_image[3]\n",
    "# 중간 테스트용 코드\n",
    "vutils.save_image(input_cropped.data[0,0],'result/single/tmp1.png')\n",
    "vutils.save_image(input_cropped.data[0,1],'result/single/tmp2.png')\n",
    "vutils.save_image(input_cropped.data[0,2],'result/single/tmp3.png')\n",
    "vutils.save_image(input_cropped.data[0,3],'result/single/tmp4.png')\n",
    "# 천공영역 뒤집기\n",
    "input_pngReverse[0] = torch.abs(png_image[3] - 1)[image_margin:image_margin+opt.ndf, image_margin:image_margin+opt.ndf]\n",
    "print(\"data loading has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = resume_epoch\n",
    "vutils.save_image(input_cropped.data[0],'tmp.png')\n",
    "\n",
    "# netG에 inferrence 실행하고 fake에 결과 저장\n",
    "fake = netG(input_cropped)\n",
    "\n",
    "# 결과 텐서에 천공영역 저장\n",
    "result_img[0,0] = input_cropped.data[0,0]\n",
    "result_img[0,1] = input_cropped.data[0,1]\n",
    "result_img[0,2] = input_cropped.data[0,2]\n",
    "vutils.save_image(result_img[0],'result/single/single_test_image(cropped)_%03d_%s' % (epoch,opt.testimg)) #천공이미지 저장\n",
    "# fake와 천공영역을 Hardmard Product 한 결과를 Add 연산하여 결과이미지에 합성, 결과이미지 저장\n",
    "result_img[0,0,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] += fake.data[0,0] * input_pngReverse[0]\n",
    "result_img[0,1,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] += fake.data[0,1] * input_pngReverse[0]\n",
    "result_img[0,2,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] += fake.data[0,2] * input_pngReverse[0]\n",
    "vutils.save_image(result_img[0],'result/single/single_test_image(mask)_%03d_%s' % (epoch,opt.testimg))\n",
    "result_img[0,0] = input_cropped.data[0,0]\n",
    "result_img[0,1] = input_cropped.data[0,1]\n",
    "result_img[0,2] = input_cropped.data[0,2]\n",
    "result_img[0,0,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] = fake.data[0,0]\n",
    "result_img[0,1,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] = fake.data[0,1]\n",
    "result_img[0,2,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] = fake.data[0,2]\n",
    "# 결과이미지가 아닌 fake 이미지 저장\n",
    "vutils.save_image(result_img[0],'result/single/single_test_image(recon)_%03d_%s' % (epoch,opt.testimg))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
