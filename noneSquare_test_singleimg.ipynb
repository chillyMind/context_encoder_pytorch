{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from modelAlpha import _netlocalWD,_netWG\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class opt():\n",
    "    def __init__(self):\n",
    "        self.dataset= 'streetview'\n",
    "        self.testimg = 'test_toon8.png'\n",
    "        self.workers=2\n",
    "        self.batchSize=64 #'input batch size')\n",
    "        self.imageSize=256 #the height / width of the input image to network')\n",
    "        self.nz=100\n",
    "        self.ngf=128\n",
    "        self.ndf=128 # center image size\n",
    "        self.nc=4\n",
    "        self.niter=1\n",
    "        self.lr=0.0002\n",
    "        self.beta1=0.5\n",
    "        self.cuda=True\n",
    "        self.ngpu=1\n",
    "        self.netG='model/netG_streetview_1.pth'\n",
    "        #self.netG='model/tmp.pth'\n",
    "        self.outf='.'\n",
    "        self.manualSeed = 0\n",
    "        self.nBottleneck=4000  # 'of dim for bottleneck of encoder')\n",
    "        self.overlapPred = 0 # 'overlapping edges')\n",
    "        self.nef = 64#'of encoder filters in first conv layer')\n",
    "        self.wtl2 = 0.998 #'0 means do not use else use with this weight')\n",
    "        self.wtlD =0.001# means do not use else use with this weight')\n",
    "        self.jittering=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netWG(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU(0.2, inplace)\n",
      "    (11): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (13): LeakyReLU(0.2, inplace)\n",
      "    (14): Conv2d(512, 4000, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (15): BatchNorm2d(4000, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (16): LeakyReLU(0.2, inplace)\n",
      "    (17): ConvTranspose2d(4000, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (18): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (19): ReLU(inplace)\n",
      "    (20): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (21): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (22): ReLU(inplace)\n",
      "    (23): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (25): ReLU(inplace)\n",
      "    (26): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (27): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (28): ReLU(inplace)\n",
      "    (29): ConvTranspose2d(128, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (30): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "netG = _netWG(opt)\n",
    "netG.apply(weights_init)\n",
    "netG.load_state_dict(torch.load(opt.netG,map_location=lambda storage, location: storage)['state_dict'])\n",
    "resume_epoch = torch.load(opt.netG)['epoch']\n",
    "netG.eval()\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading has been done\n"
     ]
    }
   ],
   "source": [
    "image_margin = int((opt.imageSize - opt.ndf)/2)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "png_image = utils.load_image(opt.testimg, opt.imageSize)\n",
    "png_image = transform(png_image)\n",
    "input_cropped = torch.FloatTensor(1, 4, opt.imageSize, opt.imageSize)\n",
    "input_pngReverse = torch.FloatTensor(4, opt.ndf, opt.ndf)\n",
    "result_img = torch.FloatTensor(1, 3, opt.imageSize, opt.imageSize)\n",
    "input_cropped = Variable(input_cropped)\n",
    "\n",
    "\n",
    "\n",
    "#input_cropped.data.resize_(png_image.size()).copy_(png_image)\n",
    "input_pngReverse[0] = png_image[3][image_margin:image_margin+opt.ndf, image_margin:image_margin+opt.ndf]\n",
    "png_imageV = Variable(png_image)\n",
    "input_cropped.data[0,0] = png_imageV.data[0]\n",
    "input_cropped.data[0,1] = png_imageV.data[1]\n",
    "input_cropped.data[0,2] = png_imageV.data[2]\n",
    "vutils.save_image(input_pngReverse[0],'result/single/cropped.png')\n",
    "input_cropped.data[0,0,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf] = input_pngReverse[0] * input_cropped.data[0,0,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf]\n",
    "input_cropped.data[0,1,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf] = input_pngReverse[0] * input_cropped.data[0,1,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf]\n",
    "input_cropped.data[0,2,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf] = input_pngReverse[0] * input_cropped.data[0,2,\n",
    "                  image_margin:image_margin+opt.ndf,\n",
    "                  image_margin:image_margin+opt.ndf]\n",
    "input_cropped.data[0,3] = png_image[3]\n",
    "vutils.save_image(input_cropped.data[0,0],'result/single/tmp1.png')\n",
    "vutils.save_image(input_cropped.data[0,1],'result/single/tmp2.png')\n",
    "vutils.save_image(input_cropped.data[0,2],'result/single/tmp3.png')\n",
    "vutils.save_image(input_cropped.data[0,3],'result/single/tmp4.png')\n",
    "input_pngReverse[0] = torch.abs(png_image[3] - 1)[image_margin:image_margin+opt.ndf, image_margin:image_margin+opt.ndf]\n",
    "print(\"data loading has been done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch = resume_epoch\n",
    "vutils.save_image(input_cropped.data[0],'tmp.png')\n",
    "\n",
    "# train with fake\n",
    "fake = netG(input_cropped)\n",
    "\n",
    "result_img[0,0] = input_cropped.data[0,0]\n",
    "result_img[0,1] = input_cropped.data[0,1]\n",
    "result_img[0,2] = input_cropped.data[0,2]\n",
    "vutils.save_image(result_img[0],'result/single/single_test_image(cropped)_%03d_%s' % (epoch,opt.testimg))\n",
    "result_img[0,0,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] += fake.data[0,0] * input_pngReverse[0]\n",
    "result_img[0,1,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] += fake.data[0,1] * input_pngReverse[0]\n",
    "result_img[0,2,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] += fake.data[0,2] * input_pngReverse[0]\n",
    "vutils.save_image(result_img[0],'result/single/single_test_image(mask)_%03d_%s' % (epoch,opt.testimg))\n",
    "result_img[0,0] = input_cropped.data[0,0]\n",
    "result_img[0,1] = input_cropped.data[0,1]\n",
    "result_img[0,2] = input_cropped.data[0,2]\n",
    "result_img[0,0,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] = fake.data[0,0]\n",
    "result_img[0,1,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] = fake.data[0,1]\n",
    "result_img[0,2,\n",
    "     image_margin:image_margin+opt.ndf,\n",
    "     image_margin:image_margin+opt.ndf] = fake.data[0,2]\n",
    "\n",
    "vutils.save_image(result_img[0],'result/single/single_test_image(recon)_%03d_%s' % (epoch,opt.testimg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
